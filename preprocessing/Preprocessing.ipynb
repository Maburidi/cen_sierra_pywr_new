{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1IYUBFeBwxYeKRzIHIXM2QDnZ-qg7J7Dz","authorship_tag":"ABX9TyNFGj4XFfBLhn01tACgObnU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypvoqq2L_VMg","executionInfo":{"status":"ok","timestamp":1737577255860,"user_tz":480,"elapsed":8972,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"outputId":"20eea4d4-6218-45b0-83d6-de92b83a3c39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting loguru\n","  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n","Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: loguru\n","Successfully installed loguru-0.7.3\n"]}],"source":["!pip install loguru\n"]},{"cell_type":"code","source":["import shutil\n","import os, sys\n","import json\n","\n","shutil.rmtree('/content/cen_sierra_pywr_new', ignore_errors=True)\n","!git clone https://github.com/Maburidi/cen_sierra_pywr_new.git\n","sys.path.insert(0,'/content/cen_sierra_pywr_new/')\n","\n","%cd /content/cen_sierra_pywr_new/\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-F1Z9-XvQdX","executionInfo":{"status":"ok","timestamp":1737577348303,"user_tz":480,"elapsed":86939,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"outputId":"be68b322-4bed-42b0-b983-32e0e4002f68"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'cen_sierra_pywr_new'...\n","remote: Enumerating objects: 15533, done.\u001b[K\n","remote: Counting objects: 100% (1233/1233), done.\u001b[K\n","remote: Compressing objects: 100% (760/760), done.\u001b[K\n","remote: Total 15533 (delta 832), reused 432 (delta 432), pack-reused 14300 (from 4)\u001b[K\n","Receiving objects: 100% (15533/15533), 955.81 MiB | 14.47 MiB/s, done.\n","Resolving deltas: 100% (7234/7234), done.\n","Updating files: 100% (1639/1639), done.\n","/content/cen_sierra_pywr_new\n"]}]},{"cell_type":"code","source":["\n","import os\n","import multiprocessing as mp\n","import argparse\n","import shutil\n","from pathlib import Path\n","import pandas as pd\n","from joblib import Parallel, delayed\n","import json\n","from loguru import logger\n","\n","from sierra.utilities import simplify_network\n","\n","\n","def create_full_natural_flow(src, dst):\n","    basin_runoff_dir = src\n","    subwats = []\n","    for filename in os.listdir(basin_runoff_dir):\n","        if '.csv' not in filename:\n","            continue\n","        path = os.path.join(basin_runoff_dir, filename)\n","        df = pd.read_csv(path, parse_dates=True, index_col=0, header=0)\n","        subwats.append(df)\n","    df = pd.concat(subwats, axis=1).sum(axis=1).to_frame()\n","    df.index.name = 'date'\n","    df.columns = ['flow']\n","\n","    outdir = dst\n","    if not os.path.exists(outdir):\n","        os.makedirs(outdir)\n","\n","    # daily\n","    df.to_csv(os.path.join(outdir, 'full_natural_flow_daily_mcm.csv'))\n","    # (df/0.0864*35.315).to_csv(os.path.join(outdir, 'full_natural_flow_daily_cfs.csv'))\n","\n","    # monthly\n","    df2 = df.resample('MS').sum()\n","    df2.to_csv(os.path.join(outdir, 'full_natural_flow_monthly_mcm.csv'))\n","\n","    # annual\n","    df3 = df2.copy()\n","    df3['WY'] = [d.year if d.month <= 9 else d.year + 1 for d in df2.index]\n","    df3 = df3.reset_index().set_index('WY').drop('date', axis=1).groupby('WY').sum()\n","    df3.to_csv(os.path.join(outdir, 'full_natural_flow_annual_mcm.csv'))\n","\n","    return\n","\n","\n","def aggregate_subwatersheds(scenario_path, basin):\n","    if not os.path.exists(scenario_path):\n","        raise Exception('Scenario path \"{}\" does not exist'.format(scenario_path))\n","\n","    basin_runoff_dir = os.path.join(scenario_path, 'runoff')\n","\n","    if not os.path.exists(basin_runoff_dir):\n","        raise Exception('Basin path \"{}\" does not exist'.format(basin_runoff_dir))\n","\n","    subwat_groups = {}\n","\n","    # collect subwatersheds\n","    model_path = '/content/cen_sierra_pywr_new/sierra/models/' + basin + '/pywr_model.json'\n","\n","    with open(model_path) as f:\n","        full_model = json.load(f)\n","\n","    model = simplify_network(full_model, scenario_path=scenario_path, delete_gauges=True, delete_observed=True,\n","                             delete_scenarios=True,\n","                             aggregate_runoff=False)\n","\n","    for n1, n2 in model['edges']:\n","        if ' Headflow' in n1:\n","            subwat_groups[n2] = subwat_groups.get(n2, []) + [n1]\n","\n","    runoff_data = []\n","    print(subwat_groups.items())\n","    for subwat_group, subwats in subwat_groups.items():\n","        subwat_group_name = '{} Inflow'.format(subwat_group)\n","        subwat_group_runoff = []\n","        for subwat in subwats:\n","            filename = 'tot_runoff_sb{}_mcm.csv'.format(subwat.split(' ')[0].split('_')[1])\n","            path = os.path.join(scenario_path, 'runoff', filename)\n","            df = pd.read_csv(path, parse_dates=True, index_col=0, header=0)\n","            subwat_group_runoff.append(df)\n","        df = pd.concat(subwat_group_runoff, axis=1).sum(axis=1).to_frame()\n","        df.index.name = 'date'\n","        df.columns = [\"flow\"]\n","\n","        outdir = os.path.join(scenario_path, 'runoff_aggregated')\n","        if not os.path.exists(outdir):\n","            os.makedirs(outdir)\n","\n","        df.to_csv(os.path.join(outdir, '{} mcm.csv'.format(subwat_group_name)))\n","\n","    return\n","\n","\n","\n","\n","\n","def create_forecasted_hydrology(scenario_path, dataset=None, default_alpha=0.2, nyears_of_record=25):\n","\n","    alphas = {}  # Note: default is zero\n","    for m in range(1, 13):\n","        alphas[m] = {}\n","        if 3 >= m <= 9:  # Mar-Sep\n","            for m2 in range(m, 9 + 1):\n","                alphas[m][m2] = 0.5 if m == 3 else 0.9\n","\n","    # Initial pre-processing\n","\n","    debug = False\n","    month_columns = ['{:02}'.format(i) for i in range(1, 13)]\n","\n","    # get source runoff data\n","    runoff_dir = 'runoff_aggregated'\n","    scenario_runoff_dir_path = os.path.join(scenario_path, runoff_dir)\n","    if dataset in ['historical', 'gcms', 'LOCA2_gcms']:\n","        src_runoff_dir_path = scenario_runoff_dir_path\n","    else:\n","        src_runoff_dir_path = os.sep.join(scenario_path.split(os.sep)[:-2] + ['historical', 'Livneh', runoff_dir])\n","    src_runoff_filenames = os.listdir(src_runoff_dir_path)\n","\n","    # create output data folder\n","    # runoff_dir_path = os.path.join(scenario_path, runoff_dir)\n","    #         print(runoff_dir)\n","    # runoff_dir_monthly = runoff_dir_path.replace(runoff_dir, 'runoff_monthly')\n","    runoff_dir_monthly_forecasts = os.path.join(scenario_path, 'runoff_monthly_forecasts')\n","    if os.path.exists(runoff_dir_monthly_forecasts):\n","        shutil.rmtree(runoff_dir_monthly_forecasts, ignore_errors=True)\n","    os.makedirs(runoff_dir_monthly_forecasts)\n","\n","    months_to_calculate = None\n","\n","    for runoff_filename in src_runoff_filenames:\n","        if '.csv' not in runoff_filename:\n","            continue\n","        src_path = os.path.join(src_runoff_dir_path, runoff_filename)\n","        logger.info('Processing ' + src_path)\n","        df = pd.read_csv(src_path, parse_dates=True, index_col=0)\n","        col = df.columns[0]\n","\n","        # Aggregate to monthly\n","        df2 = df.groupby([lambda x: x.year, lambda x: x.month]).sum()\n","        df2.index.names = ['year', 'month']\n","\n","        if months_to_calculate is None:\n","            if dataset != 'sequences':\n","                months_to_calculate = list(df2.index)\n","                earliest_year = months_to_calculate[0][0]\n","            elif dataset == 'sequences':\n","                n_years = int(scenario_path.split('_Y')[1][:2])\n","                print(n_years)\n","                months_to_calculate = []\n","                for wy in range(2025, 2025 + n_years):\n","                    for m in [10,11,12,1,2,3,4,5,6,7,8,9]:\n","                        y = wy - 1 if m >= 10 else wy\n","                        months_to_calculate.append((y, m))\n","                earliest_year = 2013 - nyears_of_record\n","            else:\n","                raise('Routine not complete for dataset {}'.format(dataset))\n","\n","        vals = []\n","        for i, (year, month) in enumerate(months_to_calculate):\n","\n","            # Monthly median\n","            start_year = max(year - nyears_of_record, earliest_year)\n","            end_year = start_year + nyears_of_record\n","            years_of_record = list(range(start_year, end_year + 1))\n","            index_slice = pd.IndexSlice[years_of_record, :]\n","            df3 = df2.loc[index_slice]\n","            df_median = df3.groupby('month').median()\n","\n","            q_next = df2[col].iloc[i:i + 12].values\n","            if len(q_next) < 12:\n","                break\n","\n","            next_months = [i + month for i in range(12)]\n","            next_months = [m if m < 13 else m - 12 for m in next_months]\n","            q_next_median = [df_median[col].loc[m] for m in next_months]\n","\n","            # CORE FORECASTING ROUTINE\n","            next_months_qfcst = []\n","            for j, m in enumerate(next_months):\n","                alpha = alphas[month].get(m, default_alpha)\n","                fcst = alpha * q_next[j] + (1 - alpha) * q_next_median[j]\n","                next_months_qfcst.append(fcst)\n","\n","            vals.append(next_months_qfcst)\n","\n","        index = pd.to_datetime(['{}-{}-01'.format(ym[0], ym[1]) for ym in months_to_calculate])\n","        df_final = pd.DataFrame(index=index[:len(vals)], data=vals, columns=month_columns)\n","        df_final.index.name = 'Date'\n","        df_final.to_csv(os.path.join(runoff_dir_monthly_forecasts, runoff_filename))\n","\n","        if debug:\n","            #             print(df3.head())\n","            #             fig, ax = plt.subplots(figsize=(12,5))\n","            #             df3.plot(ax=ax)\n","            #             plt.show()\n","            break\n","\n","\n","def calculate_WYT_P2005_P2130(scenario_path):\n","    def assign_WYT(row):\n","        if row['flow'] <= 350000:\n","            x = 1\n","        elif row['flow'] <= 676000:\n","            x = 2\n","        elif row['flow'] <= 1050000:\n","            x = 3\n","        elif row['flow'] <= 1585000:\n","            x = 4\n","        else:\n","            x = 5\n","        return x\n","\n","    def assign_WY(row):\n","        if row['date'].month in [10, 11, 12]:\n","            x = row['date'].year + 1\n","        else:\n","            x = row['date'].year\n","        return x\n","\n","    fnf_path = os.path.join(scenario_path, 'preprocessed', 'full_natural_flow_daily_mcm.csv')\n","    inflow_melones = pd.read_csv(fnf_path, names=['date', 'flow'], header=0, parse_dates=[0])  # mcm\n","    inflow_melones['WY'] = inflow_melones.apply(lambda row: assign_WY(row), axis=1)\n","    df = inflow_melones.groupby('WY')['flow'].sum() * 810.71318  # mcm to AF\n","    df1 = df.to_frame()\n","    df1['WYT'] = df1.apply(lambda row: assign_WYT(row), axis=1)\n","    df1.drop('flow', axis=1, inplace=True)\n","    out_path = os.path.join(scenario_path, 'preprocessed', 'WYT_P2005_P2130.csv')\n","    df1.to_csv(out_path)\n","\n","\n","def calculate_WYT_P2019(scenario_path):\n","    def assign_WYT(row):\n","        if row['flow'] <= 140000:\n","            x = 1\n","        elif row['flow'] <= 320000:\n","            x = 2\n","        elif row['flow'] <= 400000:\n","            x = 3\n","        elif row['flow'] <= 500000:\n","            x = 4\n","        else:\n","            x = 5\n","        return x\n","\n","    fnf_path = os.path.join(scenario_path, 'preprocessed', 'full_natural_flow_daily_mcm.csv')\n","    df = pd.read_csv(fnf_path, names=['date', 'flow'], index_col=0, header=0, parse_dates=[0])  # mcm\n","    df['year'] = df.index.year\n","    df1 = df[df.index.map(lambda x: x.month in [4, 5, 6, 7])]\n","    df2 = df1.groupby('year').sum() / 1.2335 * 1000\n","    df2.index.name = 'WY'\n","    df2['WYT'] = df2.apply(lambda row: assign_WYT(row), axis=1)\n","    df2.drop('flow', axis=1, inplace=True)\n","    outpath = os.path.join(scenario_path, 'preprocessed', 'WYT_P2019.csv')\n","    df2.to_csv(outpath)\n","\n","def calculate_peak_donnell_lake_inflow(scenario_path):\n","    dfs = []\n","    for subwat in [18, 19, 20, 21]:\n","        path = os.path.join(scenario_path, 'runoff', 'tot_runoff_sb{}_mcm.csv'.format(subwat))\n","        df = pd.read_csv(path, header=0, index_col=0, parse_dates=True, names=['date', 'flow'])\n","        dfs.append(df)\n","    df = pd.concat(dfs)\n","    df['year'] = df.index.year\n","    grouped_df = df[df.index.map(lambda x: x.month in [3, 4, 5])].groupby('year')\n","    peak_flow_df = grouped_df.idxmax()\n","    peak_flow_df.columns = ['date']\n","    # peak_flow_df['flow'] = grouped_df.max()\n","\n","    outpath = os.path.join(scenario_path, 'preprocessed', 'Donnells_Reservoir_Peak_MAM_Runoff_date.csv')\n","    peak_flow_df.to_csv(outpath)\n","\n","    return\n","\n","\n","import os\n","from datetime import datetime, timedelta\n","from itertools import product\n","import pandas as pd\n","\n","\n","def full_natural_flow_exceedance_forecast(scenario_path):\n","    \"\"\"\n","    This function calculates the full natural flow exceedance forecasts each month.\n","    Actually, for now it really does no such thing, and simply assumes perfect forecast. However, it can be updated\n","    with a real forecasting routine as needed. For now, the focus is on the Stanislaus River, Feb-Sep runoff.\n","\n","    :param basin_preprocessed_path:\n","    :param scenario:\n","    :return:\n","    \"\"\"\n","\n","\n","    # Approximate DWR's 50% exceedance flows for the basin.\n","    if 'sequences' in scenario_path:\n","        livneh_scenario_path = scenario_path.split('sequences')[0] + 'historical/Livneh'\n","        fnf_path = os.path.join(livneh_scenario_path, 'preprocessed', 'full_natural_flow_daily_mcm.csv')\n","    else:\n","        fnf_path = os.path.join(scenario_path, 'preprocessed', 'full_natural_flow_daily_mcm.csv')\n","    fnf_df = pd.read_csv(fnf_path, index_col=0, header=0, parse_dates=True)\n","\n","    months = list(range(3, 9 + 1))\n","    years = sorted(list(set([dt.year for dt in fnf_df.index])))[1:]\n","    exceedances = [50]\n","    months_exceedances = list(product(months, exceedances))\n","    years_months = list(product(years, months))\n","    columns = pd.MultiIndex.from_tuples(months_exceedances)\n","    index = pd.MultiIndex.from_tuples(years_months)\n","    fnf_all = pd.DataFrame(index=index, columns=columns)\n","    fnf_all.index.names = ['year', 'month']\n","    fnf_all.columns.names = ['month', 'exceedance']\n","    for exceedance in exceedances:\n","        for year, month in years_months:\n","\n","            ytd_monthly = []\n","\n","            # forecast year-to-go\n","            ytg_start = '{:04}-{:02}-01'.format(year, month)\n","            ytg_end = '{:04}-09-30'.format(year)\n","\n","            if month <= 6:\n","                # insert forecast routine here\n","                ytg_monthly = fnf_df[ytg_start:ytg_end].resample('MS').sum()['flow'].values\n","\n","            else:\n","                # assume perfect year-to-go forecast after June\n","                ytg_monthly = fnf_df[ytg_start:ytg_end].resample('MS').sum()['flow'].values\n","\n","            # actual to-date\n","            if month > 3:\n","                ytd_start = '{:04}-03-01'.format(year)\n","                ytd_end = (datetime(year, month, 1) - timedelta(days=1)).strftime('%Y-%m-%d')\n","                ytd_monthly = fnf_df[ytd_start:ytd_end].resample('MS').sum()['flow'].values\n","\n","            # aggregated\n","            ytd_ytg = list(ytd_monthly) + list(ytg_monthly)\n","            fnf_all.loc[(year, month), (months, exceedance)] = ytd_ytg\n","\n","        # sum across exceedance\n","        fnf_all[('sum', exceedance)] = fnf_all.loc[(years, months), (months, exceedance)].sum(axis=1)\n","\n","    outpath = os.path.join(scenario_path, 'preprocessed', 'exceedance_forecast_mcm.csv')\n","    fnf_all.to_csv(outpath)\n","\n","\n","\n","\n","FRACTIONS = dict(a=0.42, b=0.03, c=0.47, d=0.08)\n","\n","\n","def disaggregate_SJN_09_subwatershed(scenario_path):\n","    \"\"\"\n","    This scripts disaggregates the original SJN_09 subwatersheds into four smaller subcatchments.\n","    :param scenarios_path:\n","    :param scenarios:\n","    :return:\n","    \"\"\"\n","\n","    hydrology_path = os.path.join(scenario_path, 'runoff')\n","\n","    sjn09_path = os.path.join(hydrology_path, 'tot_runoff_sb09_mcm.csv')\n","    df = pd.read_csv(sjn09_path, index_col=0, header=0, parse_dates=True)\n","\n","    for subsubwat in ['a', 'b', 'c', 'd']:\n","        df2 = df * FRACTIONS[subsubwat]\n","        df2.to_csv(sjn09_path.replace('sb09', 'sb09{}'.format(subsubwat)))\n","\n","\n","\n","\n","\n","\n","def sjrrp_below_friant(src, dst):\n","    thresholds_taf = [0, 400, 670, 930, 1450, 2500]\n","    allocations_taf = [116.866, 187.785, 272.280, 330.300, 400.300, 547.400, 673.488]\n","\n","    inpath = os.path.join(src, 'full_natural_flow_annual_mcm.csv')\n","\n","    df = pd.read_csv(inpath, index_col=0, header=0)\n","\n","    wyts = []\n","    allocations_mcm = []\n","    allocation_fractions = []\n","    for i, runoff_mcm in enumerate(df['flow']):\n","        runoff_taf = runoff_mcm / 1.2335\n","        wyt = sum([1 for t in thresholds_taf if runoff_taf > t])\n","        wyts.append(wyt)\n","\n","        idx = wyt-1\n","        if wyt in [1, 2, 6]:\n","            if wyt == 1:\n","                allocation_taf = allocations_taf[0]\n","            elif wyt == 2:\n","                allocation_taf = allocations_taf[1]\n","            else:\n","                allocation_taf = allocations_taf[-1]\n","            allocation_fraction = 1\n","        else:\n","            tlow = thresholds_taf[idx]\n","            thigh = thresholds_taf[idx+1]\n","            rlow = allocations_taf[idx]\n","            rhigh = allocations_taf[idx+1]\n","            slope = (rhigh-rlow)/(thigh-tlow)\n","            allocation_taf = rlow + slope * (runoff_taf - tlow)\n","            allocation_taf_middle = (rhigh + rlow) / 2\n","            allocation_fraction = allocation_taf / allocation_taf_middle\n","\n","        allocation_mcm = allocation_taf * 1.2335\n","        allocations_mcm.append(allocation_mcm)\n","        allocation_fractions.append(allocation_fraction)\n","\n","    df2 = pd.DataFrame(index=df.index)\n","    df2['WYT'] = wyts\n","    df2['Annual allocation mcm'] = allocations_mcm\n","    df2['Allocation adjustment'] = allocation_fractions\n","\n","    outpath = os.path.join(dst, 'SJ restoration flows.csv')\n","    df2.to_csv(outpath)\n","\n","def calculate_millerton_snowmelt_inflow(source, destination):\n","\n","    # Read in the data\n","    inpath = os.path.join(source, 'full_natural_flow_daily_mcm.csv')\n","    df = pd.read_csv(inpath, index_col=0, header=0, parse_dates=True) / 1.2335 * 1000  # mcm to acre-feet\n","\n","    # Filter by month (Apr-Jul) and resample by year\n","    df2 = df[df.index.map(lambda x: x.month in [4, 5, 6, 7])].resample('Y').sum()\n","    df2.index = df2.index.map(lambda x: x.year)\n","    df2.index.name = 'year'\n","\n","    outpath = os.path.join(destination, 'inflow_MillertonLake_AprToJul_af.csv')\n","    df2.to_csv(outpath)\n","\n","    return\n","\n","def calc_WY(flow):\n","    if flow >= 450:\n","        return 1\n","    else:\n","        return 2\n","\n","def calculate_Exchequer_WYT(scenario_path):\n","    fnf_path = os.path.join(scenario_path, 'preprocessed', 'full_natural_flow_daily_mcm.csv')\n","    df = pd.read_csv(fnf_path, header=0, index_col=0, parse_dates=True, names=['flow'])\n","    df2 = df[(df.index.month >= 4) & (df.index.month <= 7)]\n","    df3 = df2.resample('Y').sum()\n","    df3 /= 1.2335  # mcm to TAF\n","\n","    # compute the water year type\n","    df3['WYT'] = df3['flow'].apply(lambda x: 1 if x >= 450 else 2)\n","    df3.index = df3.index.year\n","    df3.index.name = 'WY'\n","\n","    # create the final WYT dataframe, adding an initial WYT to the beginning\n","    wyt_df = pd.Series(index=[df3.index[0] - 1] + list(df3.index))\n","    wyt_df[wyt_df.index[0]] = 3  # assume the previous water year's type is 3\n","    wyt_df.values[1:] = df3['WYT']\n","    wyt_df.index.name = 'WY'\n","    wyt_df.name = 'WYT'\n","\n","    outpath = os.path.join(scenario_path, 'preprocessed', 'Exchequer_WYT.csv')\n","    wyt_df.to_csv(outpath)\n","\n","\n","\n","\n","basins = ['Stanislaus', 'Merced', 'Upper_San_Joaquin', 'Tuolumne']\n","\n","def calculate_sjvi(dataset, climate):\n","    root_dir = '/content/cen_sierra_pywr_new/data'\n","    dfs = []\n","    for basin in basins:\n","        full_basin_name = '{}_River'.format(basin.title())\n","\n","        path = os.path.join(root_dir, full_basin_name, 'hydrology',dataset, climate, 'preprocessed',\n","                            'full_natural_flow_daily_mcm.csv')\n","        df = pd.read_csv(path, index_col=0, header=0, parse_dates=True)\n","        dfs.append(df)\n","\n","    df_maf = pd.concat(dfs, axis=1).sum(axis=1) / 1.2335 / 1e3\n","\n","    sjvi = 3.5\n","    water_years = df_maf.resample('Y').sum().index.year\n","    sjvi_values = pd.Series(index=water_years, name='SJVI (maf)', dtype=np.float64)\n","    sjvi_values.index.name = 'Water Year'\n","    sjvi_values[water_years[0]] = sjvi\n","    for wy in water_years[1:]:\n","        apr_1 = '{}-04-01'.format(wy)\n","        jul_31 = '{}-07-31'.format(wy)\n","        oct_1 = '{}-10-01'.format(wy - 1)\n","        mar_31 = '{}-03-31'.format(wy)\n","        apr_jul_maf = df_maf[apr_1:jul_31].sum()\n","        oct_mar_maf = df_maf[oct_1:mar_31].sum()\n","        sjvi = 0.6 * apr_jul_maf + 0.2 * oct_mar_maf + 0.2 * sjvi\n","        sjvi_values[wy] = sjvi\n","\n","    outpath = os.path.join(root_dir, 'common', 'hydrology', dataset, climate)\n","    if not os.path.exists(outpath):\n","        os.makedirs(outpath)\n","    sjvi_values.to_csv(os.path.join(outpath, 'SJVI.csv'))\n","    return\n"],"metadata":{"id":"TTJ8_3Mcd4sV","executionInfo":{"status":"ok","timestamp":1737577373268,"user_tz":480,"elapsed":1047,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JSVSuASJFCbl","executionInfo":{"status":"ok","timestamp":1737578048555,"user_tz":480,"elapsed":142,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["########################## Pre-Preocessing for Tuolumne River #################################\n","###############################################################################################\n","\n","# [1] ============= Aggregating subwatersheds ---------> inputted folder: runoff / Generated folder: runoff_aggregated\n","\n","gcm_model = \"ACCESS_CM2\"\n","basin = 'tuolumne'                # Upper_san_joaquin, tuolumne, stanislaus, merced\n","basin0 = 'Tuolumne_River'         # Upper_San_Joaquin_River, Stanislaus_River, Merced_River, Tuolumne_River\n","\n","\n","scenario_path = '/content/cen_sierra_pywr_new/data/' + basin0 + '/hydrology/LOCA2_gcms/' + gcm_model\n","\n","aggregate_subwatersheds(scenario_path, basin)\n","\n","\n","# [2] ============= Creating forecasted hydrology ---------> inputted folder: runoff_aggregated  /  generates folder: runoff_monthly_forecasts\n","\n","\n","scenario_path = '/content/cen_sierra_pywr_new/data/'+ basin0 + '/hydrology/LOCA2_gcms/' +  gcm_model\n","create_forecasted_hydrology(scenario_path, dataset='LOCA2_gcms', default_alpha=0.2, nyears_of_record=25)\n","\n","\n","\n","# [3] ============ Creating full natural flow ======> inputted folder: runoff / generated folder: preprocessed\n","\n","src = os.path.join(scenario_path, 'runoff_aggregated')\n","preprocessed_path = os.path.join(scenario_path, 'preprocessed')\n","dst = preprocessed_path\n","\n","# run the function\n","create_full_natural_flow(src, dst)\n","\n"],"metadata":{"id":"aV731oB_xQCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################## Pre-Preocessing for MERCED River ###################################\n","###############################################################################################\n","\n","gcm_model = \"ACCESS_CM2\"\n","basin = 'merced'                # Upper_san_joaquin, tuolumne, stanislaus, merced\n","basin0 = 'Merced_River'         # Upper_San_Joaquin_River, Stanislaus_River, Merced_River, Tuolumne_River\n","scenario_path = '/content/cen_sierra_pywr_new/data/' + basin0 + '/hydrology/LOCA2_gcms/' + gcm_model\n","\n","\n","\n","# [1] ============= Aggregating subwatersheds ---------> inputted folder: runoff / Generated folder: runoff_aggregated\n","\n","aggregate_subwatersheds(scenario_path, basin)\n","\n","# [2] ============= Creating forecasted hydrology ---------> inputted folder: runoff_aggregated  /  generates folder: runoff_monthly_forecasts\n","\n","create_forecasted_hydrology(scenario_path, dataset='LOCA2_gcms', default_alpha=0.2, nyears_of_record=25)\n","\n","# [3] ============ Creating full natural flow ======> inputted folder: runoff / generated folder: preprocessed\n","\n","src = os.path.join(scenario_path, 'runoff_aggregated')\n","preprocessed_path = os.path.join(scenario_path, 'preprocessed')\n","dst = preprocessed_path\n","\n","# run the function\n","create_full_natural_flow(src, dst)\n","\n","# [4] ============ calculate_Exchequer_WYT =====> inputted folder: preprocessed / generated file: preprocessed/Exchequer_WYT.csv\n","\n","calculate_Exchequer_WYT(scenario_path)\n","\n"],"metadata":{"id":"y8eZPy2XZzGk","executionInfo":{"status":"ok","timestamp":1737577966217,"user_tz":480,"elapsed":152,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e5c5d5a-973f-4b9a-bada-fa3997a0ca26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_items([])\n"]}]},{"cell_type":"code","source":["########################## Pre-Preocessing for Stainslus River ###################################\n","###############################################################################################\n","\n","# [1] ============= Aggregating subwatersheds ---------> inputted folder: runoff / Generated folder: runoff_aggregated\n","\n","gcm_model = \"ACCESS_CM2\"\n","basin = 'stanislaus'                # Upper_san_joaquin, tuolumne, stanislaus, merced\n","basin0 = 'Stanislaus_River'         # Upper_San_Joaquin_River, Stanislaus_River, Merced_River, Tuolumne_River\n","\n","\n","scenario_path = '/content/cen_sierra_pywr_new/data/' + basin0 + '/hydrology/LOCA2_gcms/' + gcm_model\n","\n","aggregate_subwatersheds(scenario_path, basin)\n","\n","# [2] ============= Creating forecasted hydrology ---------> inputted folder: runoff_aggregated  /  generates folder: runoff_monthly_forecasts\n","\n","create_forecasted_hydrology(scenario_path, dataset='LOCA2_gcms', default_alpha=0.2, nyears_of_record=25)\n","\n","# [3] ============ Creating full natural flow ======> inputted folder: runoff / generated folder: preprocessed\n","\n","src = os.path.join(scenario_path, 'runoff_aggregated')\n","preprocessed_path = os.path.join(scenario_path, 'preprocessed')\n","dst = preprocessed_path\n","create_full_natural_flow(src, dst)\n","\n","# [4] ============ calculate other files =====> inputted folder: preprocessed / generated file are in: preprocessed\n","\n","calculate_WYT_P2005_P2130(scenario_path)\n","calculate_WYT_P2019(scenario_path)\n","calculate_peak_donnell_lake_inflow(scenario_path)\n","full_natural_flow_exceedance_forecast(scenario_path)\n","\n"],"metadata":{"id":"9ftcquBfSB4R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################## Pre-Processing for Upper San Joaquin River #########################\n","###############################################################################################\n","\n","gcm_model = \"ACCESS_CM2\"\n","basin = 'upper_san_joaquin'                # Upper_san_joaquin, tuolumne, stanislaus, merced\n","basin0 = 'Upper_San_Joaquin_River'         # Upper_San_Joaquin_River, Stanislaus_River, Merced_River, Tuolumne_River\n","\n","\n","scenario_path = '/content/cen_sierra_pywr_new/data/' + basin0 + '/hydrology/LOCA2_gcms/' + gcm_model\n","\n","# [0] ============= disaggregate SJN_09 subwatershed:   Inputted folder: runoff / Generated files: four extra files saved in runoff folder\n","\n","disaggregate_SJN_09_subwatershed(scenario_path)\n","\n","# [1] ============= Aggregating subwatersheds ---------> inputted folder: runoff / Generated folder: runoff_aggregated\n","\n","aggregate_subwatersheds(scenario_path, basin)\n","\n","# [2] ============= Creating forecasted hydrology ---------> inputted folder: runoff_aggregated  /  generates folder: runoff_monthly_forecasts\n","\n","create_forecasted_hydrology(scenario_path, dataset='LOCA2_gcms', default_alpha=0.2, nyears_of_record=25)\n","\n","# [3] ============ Creating full natural flow ======> inputted folder: runoff / generated folder: preprocessed\n","\n","src = os.path.join(scenario_path, 'runoff_aggregated')\n","preprocessed_path = os.path.join(scenario_path, 'preprocessed')\n","dst = preprocessed_path\n","create_full_natural_flow(src, dst)\n","\n","# [4] ============ calculate other files =====> inputted folder: preprocessed / generated file are in: preprocessed\n","src = dst = preprocessed_path\n","sjrrp_below_friant(src, dst)\n","calculate_millerton_snowmelt_inflow(src, dst)\n"],"metadata":{"id":"qHUKZ0fJl-oB","executionInfo":{"status":"ok","timestamp":1737582725903,"user_tz":480,"elapsed":574,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# rename files in folders, like - by _ and . ny nothing and \" \" by \"_\"\n","\n","directory = \"/content/cen_sierra_pywr_new/data/Upper_San_Joaquin_River/hydrology/LOCA2_gcms/ACCESS_CM2/runoff_aggregated\"\n","\n","for filename in os.listdir(directory):\n","    if os.path.isfile(os.path.join(directory, filename)):\n","        name, ext = os.path.splitext(filename)\n","        new_name = name.replace(\" \", \"_\").replace(\".\", \"\").replace(\"-\", \"_\") + ext\n","        os.rename(\n","            os.path.join(directory, filename),\n","            os.path.join(directory, new_name)\n","        )\n","\n","print(\"Renaming completed!\")\n"],"metadata":{"id":"EJtOppjldlZC","executionInfo":{"status":"ok","timestamp":1737583951677,"user_tz":480,"elapsed":186,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5eb9c20-0b74-4d70-d953-f479586b358b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Renaming completed!\n"]}]},{"cell_type":"code","source":["# Save the contents\n","\n","from google.colab import files\n","\n","shutil.make_archive('preprocessed', 'zip', '/content/cen_sierra_pywr_new/data/Upper_San_Joaquin_River/hydrology/LOCA2_gcms/ACCESS_CM2/preprocessed')\n","files.download('preprocessed.zip')\n"],"metadata":{"id":"U_L0l3IZ-vio","executionInfo":{"status":"ok","timestamp":1737584198605,"user_tz":480,"elapsed":388,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"2141d972-5833-4ba6-bc8b-bd279638f1ff"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5a92ff11-2ea4-4521-bf3b-eb718986f89b\", \"preprocessed.zip\", 390392)"]},"metadata":{}}]},{"cell_type":"code","source":["########################### Calculate the SJVI INDEX ################################\n","\n","dataset = \"LOCA2_gcms\"\n","climate = \"ACCESS_CM2\"\n","calculate_sjvi(dataset, climate)\n"],"metadata":{"id":"JW9iDjBlFjoM","executionInfo":{"status":"ok","timestamp":1737586367127,"user_tz":480,"elapsed":367,"user":{"displayName":"Mohammed Aburidi","userId":"13724591464373908327"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"589db2b1-5684-4796-bbbf-0414c01ec001"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-acf206a28fc6>:21: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n","  water_years = df_maf.resample('Y').sum().index.year\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"6WW2wbt7mexQ"}}]}